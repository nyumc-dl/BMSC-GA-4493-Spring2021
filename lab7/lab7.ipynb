{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 7: Optimization and Regularization \n",
    "\n",
    "Authored by Nithish Addepalli and Ren Yi, 3-26-2020;\n",
    "Modified by Runyu Hong, 3-25-2021;\n",
    "\n",
    "The goal of this lab is to learn how to apply different regularization and optimization strategies in PyTorch using MNIST data.\n",
    "\n",
    "Here is a list of the techniques we've covered in class\n",
    "- Optimization\n",
    "    - SGD\n",
    "    - SGD (with momentum)\n",
    "    - Nesterov momentum\n",
    "    - AdaGrad\n",
    "    - RMSProp\n",
    "    - Adam\n",
    "- Some regularization\n",
    "    - Dropout\n",
    "    - Batch normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import copy\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize necessary parameters\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# if torch.cuda.is_available():\n",
    "#     torch.cuda.manual_seed(717)\n",
    "\n",
    "seed = 345\n",
    "batch_size = {'train': 64,\n",
    "              'val': 1000}\n",
    "input_size = 28 * 28\n",
    "output_size = 10\n",
    "n_feature = 3\n",
    "optim_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Loader (5-10 mins loading time)\n",
    "trainset = datasets.MNIST('data', train=True, download=True,\n",
    "                          transform=transforms.Compose([\n",
    "                              transforms.ToTensor(),\n",
    "                              transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "testset = datasets.MNIST('data', train=False, \n",
    "                         transform=transforms.Compose([\n",
    "                             transforms.ToTensor(),\n",
    "                             transforms.Normalize((0.1307,), (0.3081,))]))\n",
    "\n",
    "mnist_datasets = {'train': trainset, \n",
    "            'val': testset}\n",
    "dataset_sizes = {x: len(mnist_datasets[x]) for x in ['train', 'val']}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(mnist_datasets[x], batch_size=batch_size[x], shuffle=True)\n",
    "              for x in ['train', 'val']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## show some images\n",
    "plt.figure(figsize = (12, 8))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    image, _ = dataloaders['train'].dataset.__getitem__(i)\n",
    "    plt.imshow(image.squeeze().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, num_epochs=1, verbose = True, print_every = 100):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "    loss_dict = {'train': [], 'val': []}\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train(True)  # Set model to training mode\n",
    "            else:\n",
    "                model.train(False)  # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for batch_idx, data in enumerate(dataloaders[phase]):\n",
    "                # get the inputs\n",
    "                inputs, labels = data\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "                \n",
    "                # backward + optimize only if in training phase\n",
    "                if phase == 'train':\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size()[0]\n",
    "                running_corrects += torch.sum(preds == labels).item()\n",
    "                \n",
    "                if verbose and batch_idx % print_every == 0:\n",
    "                    print('Train set | epoch: {:3d} | {:6d}/{:6d} batches | Loss: {:6.4f}'.format(\n",
    "                        epoch, batch_idx * len(inputs), len(dataloaders[phase].dataset), loss.item()))\n",
    "                    loss_dict[phase].append(running_loss/((batch_idx + 1) * len(inputs)))\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects / dataset_sizes[phase]\n",
    "            loss_dict[phase].append(epoch_loss)\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model, loss_dict\n",
    "\n",
    "def populate_result(dictionary, method, train_loss, val_loss):\n",
    "    dictionary[method] = {}\n",
    "    dictionary[method]['train_loss'] = np.array(train_loss)\n",
    "    dictionary[method]['val_loss'] = np.array(val_loss)\n",
    "    \n",
    "def plot_loss(result, loss='train_loss', ylim=None):\n",
    "    plt.plot(result['Baseline'][loss], label='Baseline')\n",
    "    for k in result.keys():\n",
    "        if k != 'Baseline':\n",
    "            plt.plot(result[k][loss], label=k)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(ylim)\n",
    "    plt.xlabel('Steps')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_best_loss(result, loss='val_loss'):\n",
    "    labels = ['Baseline']\n",
    "    acc = [np.max(result['Baseline'][loss])]\n",
    "    for k in result.keys():\n",
    "        if k != 'Baseline':\n",
    "            labels.append(k)\n",
    "            acc.append(np.max(result[k][loss]))\n",
    "\n",
    "    x = np.arange(len(labels))\n",
    "    plt.barh(x, acc)\n",
    "    plt.yticks(x, labels)\n",
    "    plt.xlabel('Loss')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline method\n",
    "\n",
    "We will later show how different optimization and regularization techniques can improve baseline model performance. But first,\n",
    "1. What's our baseline model architecture?\n",
    "2. What's the optimization method used to train the baseline model?\n",
    "3. How does this optimization method update its parameters.\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\eta \\nabla J(\\theta_{t})$$\n",
    "where $\\eta$ denotes the learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Baseline(nn.Module):\n",
    "    def __init__(self, n_feature, output_size):\n",
    "        super(Baseline, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.SGD(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer)\n",
    "populate_result(optim_results, 'Baseline', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fancier Optimization\n",
    "\n",
    "Please complete the following cells and train the \"Baseline\" model using different optimization methods. Keep the learning rate (initial learning rate) as 0.01. Plot the loss. Which one works the best?\n",
    "\n",
    "https://pytorch.org/docs/stable/optim.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SGD with momentum\n",
    "1. How does SGD with momentum update its parameters?\n",
    "$$v_{t+1} = \\rho v_{t} + \\nabla J(\\theta_{t})$$\n",
    "$$\\theta_{t+1} = \\theta_{t} - \\eta v_{t+1}$$\n",
    "where $v$ and $\\rho$ denote velocity and momentum, respectively.\n",
    "2. Check out the documentation for SGD in PyTorch and complete the code below (Set momentum=0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = _ ### Complete the line ###\n",
    "optimizer = _ ### Complete the line ###\n",
    "model, loss_dict = _ ### Complete the line ###\n",
    "populate_result(optim_results, 'SGD_momentum', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nesterov momentum\n",
    "\n",
    "Make a minor change in the above code to apply Nesterov momentum (Set momentum=0.5). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = _ ### Complete the line ###\n",
    "optimizer = _ ### Complete the line ###\n",
    "model, loss_dict = _ ### Complete the line ###\n",
    "populate_result(optim_results, 'Nesterov_momentum', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AdaGrad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = _ ### Complete the line ###\n",
    "optimizer = _ ### Complete the line ###\n",
    "model, loss_dict = _ ### Complete the line ###\n",
    "populate_result(optim_results, 'Adagrad', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RMSprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = _ ### Complete the line ###\n",
    "optimizer = _ ### Complete the line ###\n",
    "model, loss_dict = _ ### Complete the line ###\n",
    "populate_result(optim_results, 'RMSprop', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = _ ### Complete the line ###\n",
    "optimizer = _ ### Complete the line ###\n",
    "model, loss_dict = _ ### Complete the line ###\n",
    "populate_result(optim_results, 'Adam', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization methods training results\n",
    "plot_loss(optim_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization methods validation results\n",
    "plot_best_loss(optim_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "https://pytorch.org/docs/stable/nn.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In order to see effects of regularization on validation set, we need to make some slight modification\n",
    "\n",
    "## Smaller training set\n",
    "mnist_datasets['train'].data = mnist_datasets['train'].data[:600]\n",
    "dataset_sizes = {x: len(mnist_datasets[x]) for x in ['train', 'val']}\n",
    "dataloaders['train'] = torch.utils.data.DataLoader(mnist_datasets['train'], \n",
    "                                                   batch_size=batch_size['train'], shuffle=True)\n",
    "\n",
    "## Longer training epochs\n",
    "train_epochs = 25\n",
    "verbose = False\n",
    "reg_results = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baseline model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = Baseline(n_feature, output_size).to(device)\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer, num_epochs=train_epochs, verbose=verbose)\n",
    "populate_result(reg_results, 'Baseline', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L1/L2 regularization\n",
    "\n",
    "L2 regularization is included in most optimizers in PyTorch and can be controlled with the __weight_decay__ parameter.\n",
    "For L1 regularization, check out this post: https://discuss.pytorch.org/t/simple-l2-regularization/139"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Dropout Layer\n",
    "\n",
    "1. Check out documentations for __nn.Dropout()__. Modify the Baseline model and add dropout layer to the fully connected layers.\n",
    "2. Optionally, you may also check out documentations for __nn.Dropout2d()__ to learn how to add Dropout layer to convolution layers.\n",
    "3. Set the dropout rate to 0.3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNet(nn.Module):\n",
    "    def __init__(self, input_size, n_feature, output_size, dropout_rate=0.5):\n",
    "        super(DropoutNet, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.ReLU(),\n",
    "                                   nn.MaxPool2d(2))\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = _ ### Complete the line ###\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer, num_epochs=train_epochs, verbose=verbose)\n",
    "populate_result(reg_results, 'Dropout', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding Batch Normalization\n",
    "Implement batch normalization in __BatchnormNet__. Think about where you may want to insert the batch normalization layer. (nn.BatchNorm2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BatchnormNet(nn.Module):\n",
    "    def __init__(self, n_feature, output_size):\n",
    "        super(BatchnormNet, self).__init__()\n",
    "        self.n_feature = n_feature\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(1, n_feature, kernel_size=5),\n",
    "                                   nn.MaxPool2d(2),\n",
    "                                   nn.ReLU())\n",
    "        \n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(n_feature, n_feature, kernel_size=5),\n",
    "                                   nn.MaxPool2d(2),\n",
    "                                   nn.ReLU())\n",
    "        self.fc = nn.Sequential(nn.Linear(n_feature*4*4, 50),\n",
    "                                nn.ReLU(),\n",
    "                                nn.Linear(50, 10))\n",
    "\n",
    "\n",
    "    def forward(self, x, verbose=False):\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = x.view(-1, self.n_feature*4*4)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = _ ### Complete the line ###\n",
    "optimizer = optim.Adam(model.parameters(),lr=0.01)\n",
    "model, loss_dict = train_model(model, optimizer, num_epochs=train_epochs, verbose=verbose)\n",
    "populate_result(reg_results, 'Batchnorm', loss_dict['train'], loss_dict['val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(reg_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you define the models correctly, you should see that validation loss using Dropout and Batchnorm start to catch up with the loss of baseline model after baseline model overfits training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_loss(reg_results, loss = 'val_loss', ylim=(0.1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: parameter tunning\n",
    "\n",
    "We've introduced multiple regularization and optimization techniques to improve your model. How can you combine these techniques and perform grid search to find out a set of parameters that maximize your model performance on validation set? Are there other model architectures you'd like to try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If time allowed, build your own model and explore more options/combinations ###"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
